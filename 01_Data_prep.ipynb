{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c29aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION & MAPPINGS\n",
    "# ==========================================\n",
    "\n",
    "# UPDATE!!!: Local Directory Paths\n",
    "\n",
    "DATA_DIR = r'data/raw'\n",
    "OUTPUT_DIR = r'data/processed'\n",
    "IMG_DIR = r'data/visuals'\n",
    "OUTPUT_FILE = '01_Data_Prep_Master.csv'\n",
    "\n",
    "# hierarchy: check regions first to catch \"Exports to China\" before sector logic applies\n",
    "GEO_MAP = {\n",
    "    'ASEAN': ['asean', 'southeast asia', 'brunei', 'cambodia', 'indonesia', 'lao', 'malaysia', 'myanmar', 'philippines', 'thailand', 'vietnam'],\n",
    "    'European Union': ['european union', 'eu-27', 'belgium', 'france', 'germany', 'greece', 'italy', 'netherlands', 'spain', 'sweden', 'ireland', 'denmark', 'finland', 'portugal', 'luxembourg', 'cyprus'],\n",
    "    'Greater China': ['china', 'hong kong', 'taiwan', 'macau'],\n",
    "    'North America': ['usa', 'united states', 'america', 'canada', 'mexico'],\n",
    "    'South Asia': ['india', 'bangladesh', 'pakistan', 'sri lanka', 'south asia'],\n",
    "    'East Asia': ['japan', 'korea', 'north asia', 'east asia'],\n",
    "    'Oceania': ['australia', 'new zealand', 'oceania', 'papua new guinea', 'marshall islands'],\n",
    "    'Middle East': ['uae', 'united arab emirates', 'saudi', 'qatar', 'kuwait', 'israel', 'iran', 'west asia', 'turkiye', 'turkey'],\n",
    "    'Europe (Non-EU)': ['united kingdom', 'uk', 'switzerland', 'norway', 'russia', 'europe'],\n",
    "    'Africa': ['africa', 'south africa', 'egypt', 'nigeria', 'mauritius', 'liberia'],\n",
    "    'South America': ['brazil', 'chile', 'peru', 'panama', 'argentina'],\n",
    "    'Offshore / Islands': ['bermuda', 'cayman', 'virgin islands']\n",
    "}\n",
    "\n",
    "# priority mapping for SSIC 2020 alignment\n",
    "SECTOR_MAP = [\n",
    "    ('Agriculture & Primary Ind.', ['agriculture', 'fishing', 'quarrying']),\n",
    "    ('Information & Communications', ['info', 'comm', 'tele', 'computer', 'software', 'media', 'audio-visual', 'publishing']),\n",
    "    ('Wholesale & Retail Trade', ['wholesale', 'retail', 'trade', 'shopping']),\n",
    "    ('Real Estate', ['real estate', 'ownership', 'property', 'leasing']),\n",
    "    ('Transportation & Storage', ['transport', 'storage', 'freight', 'logistics', 'postal', 'courier', 'shipping', 'air', 'sea', 'travel', 'passenger']),\n",
    "    ('Manufacturing', ['manufact', 'precision', 'chemical', 'biomedical']),\n",
    "    ('Construction', ['construct']),\n",
    "    ('Finance & Insurance', ['financ', 'insur', 'bank', 'fisim', 'fund']),\n",
    "    ('Professional Services', ['profession', 'legal', 'account', 'consult', 'architect', 'engineer', 'business', 'headquarters']),\n",
    "    ('Admin & Support Services', ['admin', 'support', 'employment agency', 'security']),\n",
    "    ('Education & Health', ['education', 'health', 'medical', 'social', 'hospital']),\n",
    "    ('Arts & Recreation', ['arts', 'entertainment', 'recreation', 'sightseeing']),\n",
    "    ('Accommodation & Food Services', ['accom', 'food', 'beverage', 'hotel', 'restaurant']),\n",
    "    ('Public Administration', ['public admin', 'government', 'defence']),\n",
    "    ('Other Services', ['other services', 'personal services', 'repair', 'maintenance']),\n",
    "    # Macros kept for context, separated in downstream modeling\n",
    "    ('Macro: GDP', ['gdp', 'gross domestic']),\n",
    "    ('Macro: Labor', ['employment', 'jobseekers', 'placements']),\n",
    "    ('Macro: Tourism', ['tourism', 'visitor']),\n",
    "    ('Macro: Trade Flow', ['export', 'import'])\n",
    "]\n",
    "\n",
    "# keywords that indicate header junk or aggregates I want to drop to avoid double counting\n",
    "NOISE_KEYWORDS = [\n",
    "    'data are', 'refer to', 'contact', 'generated', 'note:', 'nil', 'not available',\n",
    "    'definitions', 'values are shown', 'numbers may not add', 'notation:', 'notes',\n",
    "    'place of residence', 'unknown', 'male', 'female', 'not elsewhere classified',\n",
    "    'not elsewhere specified', 'services producing', 'goods producing', 'value added',\n",
    "    'taxes on', 'basic price', 'market prices'\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def parse_universal_date(date_str):\n",
    "    \"\"\"Parses mixed frequencies (Quarterly, Monthly, Annual) into standard Timestamps.\"\"\"\n",
    "    try:\n",
    "        date_str = str(date_str).strip()\n",
    "\n",
    "        # Handle Quarterly: \"2023 1Q\" -> 2023-03-31\n",
    "        if 'Q' in date_str:\n",
    "            parts = date_str.split()\n",
    "            if len(parts) == 2:\n",
    "                year, quarter = int(parts[0]), int(parts[1][0])\n",
    "                month = quarter * 3\n",
    "                return pd.Timestamp(year=year, month=month, day=1) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "        # Handle Monthly: \"2023 Jan\"\n",
    "        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        if any(m in date_str for m in months):\n",
    "            return pd.to_datetime(date_str) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "        # Handle Annual: \"2023\" -> 2023-12-31\n",
    "        # forcing annual data to Dec 31 avoids ambiguous offsets when merging with quarterly data\n",
    "        if date_str.isdigit() and len(date_str) == 4:\n",
    "            return pd.Timestamp(year=int(date_str), month=12, day=31)\n",
    "\n",
    "        return pd.NaT\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def ingest_dos_csv(file_path, filename):\n",
    "    \"\"\"Detects variable headers and melts wide-format DOS csvs.\"\"\"\n",
    "    # scan first 20 lines for metadata anchor \"Data Series\"\n",
    "    header_row = 0\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines[:20]):\n",
    "            if \"Data Series\" in line or \"Variables\" in line:\n",
    "                header_row = i\n",
    "                break\n",
    "\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1', header=header_row, on_bad_lines='skip')\n",
    "    df = df.rename(columns={df.columns[0]: 'raw_variable'})\n",
    "    df = df.dropna(subset=['raw_variable'])\n",
    "\n",
    "    # melt (N, T) -> (N*T, 1) for database ingestion\n",
    "    id_vars = ['raw_variable']\n",
    "    value_vars = [c for c in df.columns if c not in id_vars]\n",
    "    df_long = df.melt(id_vars=id_vars, value_vars=value_vars, var_name='raw_date', value_name='value')\n",
    "    df_long['source_file'] = filename\n",
    "    return df_long\n",
    "\n",
    "def get_region(raw_name):\n",
    "    name = str(raw_name).lower()\n",
    "    for region, keywords in GEO_MAP.items():\n",
    "        if any(k in name for k in keywords):\n",
    "            return region\n",
    "    return \"Singapore / Total\"\n",
    "\n",
    "def get_sector(raw_name):\n",
    "    name = str(raw_name).strip()\n",
    "    name_lower = name.lower()\n",
    "\n",
    "    # noise filtering\n",
    "    if any(k in name_lower for k in NOISE_KEYWORDS) or len(name) > 100:\n",
    "        return \"REMOVE\"\n",
    "\n",
    "    for target, keywords in SECTOR_MAP:\n",
    "        if any(k in name_lower for k in keywords):\n",
    "            return target\n",
    "\n",
    "    # fallback: if it's a country name, tag as trade flow\n",
    "    if get_region(name) != \"Singapore / Total\":\n",
    "        return \"Cross-Border / Trade Partner\"\n",
    "\n",
    "    return \"Other Industries\"\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "# 1. Manual Data Injection (IMDA)\n",
    "# unstable source format necessitates hardcoding this critical digital adoption data\n",
    "imda_raw = {\n",
    "    'raw_variable': (\n",
    "        ['Business Usage of Computers'] * 6 +\n",
    "        ['Business Usage of Internet'] * 6 +\n",
    "        ['Business Usage of Cloud Computing Services'] * 6 +\n",
    "        ['Business Usage of E-Payment'] * 6 +\n",
    "        ['Business Usage of Data Analytics'] * 6 +\n",
    "        ['Business Usage of Artificial Intelligence'] * 6\n",
    "    ),\n",
    "    'year_str': ['2019', '2020', '2021', '2022', '2023', '2024'] * 6,\n",
    "    'value': [\n",
    "        0.92, 0.91, 0.92, 0.94, 0.94, 0.94, # Computers\n",
    "        0.95, 0.93, 0.93, 0.96, 0.96, 0.97, # Internet\n",
    "        0.22, 0.20, 0.27, 0.27, 0.31, 0.36, # Cloud\n",
    "        0.81, 0.85, 0.87, 0.93, 0.94, 0.94, # E-Payment\n",
    "        0.11, 0.10, 0.13, 0.13, 0.14, 0.17, # Data Analytics\n",
    "        0.04, 0.04, 0.03, 0.04, 0.04, 0.15  # AI\n",
    "    ]\n",
    "}\n",
    "df_imda = pd.DataFrame(imda_raw)\n",
    "df_imda['source_file'] = 'Manual_Input_IMDA_Usage'\n",
    "df_imda['date_obj'] = pd.to_datetime(df_imda['year_str'] + '-12-31')\n",
    "df_imda = df_imda.drop(columns=['year_str'])\n",
    "\n",
    "# 2. Universal Ingestion Loop\n",
    "dfs_to_concat = [df_imda]\n",
    "\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.csv') and 'ERI_' not in file:\n",
    "            try:\n",
    "                full_path = os.path.join(root, file)\n",
    "                df_part = ingest_dos_csv(full_path, file)\n",
    "\n",
    "                # parsing dates immediately allows early filtering of garbage rows\n",
    "                df_part['date_obj'] = df_part['raw_date'].apply(parse_universal_date)\n",
    "                valid_rows = df_part.dropna(subset=['date_obj'])\n",
    "\n",
    "                if not valid_rows.empty:\n",
    "                    dfs_to_concat.append(valid_rows)\n",
    "            except Exception as e:\n",
    "                # I'm silencing errors here to allow the batch to proceed;\n",
    "                # bad files are usually just metadata/readme files\n",
    "                pass\n",
    "\n",
    "# 3. Consolidation & Cleaning\n",
    "df_master = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "df_master['value'] = pd.to_numeric(df_master['value'], errors='coerce')\n",
    "\n",
    "# apply taxonomies\n",
    "df_master['clean_sector'] = df_master['raw_variable'].apply(get_sector)\n",
    "df_master['clean_region'] = df_master['raw_variable'].apply(get_region)\n",
    "\n",
    "# strict filtering\n",
    "df_master = df_master[df_master['clean_sector'] != \"REMOVE\"].copy()\n",
    "df_master = df_master.dropna(subset=['value'])\n",
    "\n",
    "# 4. Final Formatting\n",
    "df_master['year'] = df_master['date_obj'].dt.year\n",
    "df_master['quarter'] = df_master['date_obj'].dt.quarter\n",
    "df_master['frequency'] = 'quarterly' # default granularity\n",
    "\n",
    "# standardize text\n",
    "df_master['clean_sector'] = df_master['clean_sector'].str.strip().str.title()\n",
    "df_master['clean_region'] = df_master['clean_region'].str.strip().str.title()\n",
    "\n",
    "# final schema enforcement\n",
    "cols_order = ['clean_sector', 'clean_region', 'year', 'quarter', 'frequency', 'raw_variable', 'value', 'source_file']\n",
    "df_master = df_master[cols_order].sort_values(by=['clean_sector', 'clean_region', 'year', 'quarter'])\n",
    "\n",
    "# 5. Save Artifact\n",
    "# UPDATED: Save to the Output folder\n",
    "output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)\n",
    "df_master.to_csv(output_path, index=False)\n",
    "print(f\"Saved artifact to Local Drive: {output_path}\")\n",
    "print(f\"Final Shape: {df_master.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
